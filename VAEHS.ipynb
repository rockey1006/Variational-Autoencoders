{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-e49f77e69220>:8: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\wruoq\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\wruoq\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\wruoq\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\wruoq\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\wruoq\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from hyperspherical_vae.distributions import VonMisesFisher\n",
    "from hyperspherical_vae.distributions import HypersphericalUniform\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('data/', one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelVAE(object):\n",
    "\n",
    "    def __init__(self, x, h_dim, z_dim, activation=tf.nn.relu, distribution='normal'):\n",
    "        \"\"\"\n",
    "        ModelVAE initializer\n",
    "\n",
    "        :param x: placeholder for input\n",
    "        :param h_dim: dimension of the hidden layers\n",
    "        :param z_dim: dimension of the latent representation\n",
    "        :param activation: callable activation function\n",
    "        :param distribution: string either `normal` or `vmf`, indicates which distribution to use\n",
    "        \"\"\"\n",
    "        self.x, self.h_dim, self.z_dim, self.activation, self.distribution = x, h_dim, z_dim, activation, distribution\n",
    "\n",
    "        self.z_mean, self.z_var = self._encoder(self.x)\n",
    "\n",
    "        if distribution == 'normal':\n",
    "            self.q_z = tf.distributions.Normal(self.z_mean, self.z_var)\n",
    "        elif distribution == 'vmf':\n",
    "            self.q_z = VonMisesFisher(self.z_mean, self.z_var)\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        self.z = self.q_z.sample()\n",
    "\n",
    "        self.logits = self._decoder(self.z)\n",
    "\n",
    "    def _encoder(self, x):\n",
    "        \"\"\"\n",
    "        Encoder network\n",
    "\n",
    "        :param x: placeholder for input\n",
    "        :return: tuple `(z_mean, z_var)` with mean and concentration around the mean\n",
    "        \"\"\"\n",
    "        # dynamic binarization\n",
    "        x = tf.cast(tf.greater(x, tf.random_uniform(shape=tf.shape(x), dtype=x.dtype)), dtype=x.dtype)\n",
    "        \n",
    "        # 2 hidden layers encoder\n",
    "        h0 = tf.layers.dense(x, units=self.h_dim * 2, activation=self.activation)\n",
    "        h1 = tf.layers.dense(h0, units=self.h_dim, activation=self.activation)\n",
    "\n",
    "        if self.distribution == 'normal':\n",
    "            # compute mean and std of the normal distribution\n",
    "            z_mean = tf.layers.dense(h1, units=self.z_dim, activation=None)\n",
    "            z_var = tf.layers.dense(h1, units=self.z_dim, activation=tf.nn.softplus)\n",
    "        elif self.distribution == 'vmf':\n",
    "            # compute mean and concentration of the von Mises-Fisher\n",
    "            z_mean = tf.layers.dense(h1, units=self.z_dim, activation=lambda x: tf.nn.l2_normalize(x, axis=-1))\n",
    "            # the `+ 1` prevent collapsing behaviors\n",
    "            z_var = tf.layers.dense(h1, units=1, activation=tf.nn.softplus) + 1\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        return z_mean, z_var\n",
    "\n",
    "    def _decoder(self, z):\n",
    "        \"\"\"\n",
    "        Decoder network\n",
    "\n",
    "        :param z: tensor, latent representation of input (x)\n",
    "        :return: logits, `reconstruction = sigmoid(logits)`\n",
    "        \"\"\"\n",
    "        # 2 hidden layers decoder\n",
    "        h2 = tf.layers.dense(z, units=self.h_dim, activation=self.activation)\n",
    "        h2 = tf.layers.dense(h2, units=self.h_dim * 2, activation=self.activation)\n",
    "        logits = tf.layers.dense(h2, units=self.x.shape[-1], activation=None)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerVAE(object):\n",
    "\n",
    "    def __init__(self, model, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        OptimizerVAE initializer\n",
    "\n",
    "        :param model: a model object\n",
    "        :param learning_rate: float, learning rate of the optimizer\n",
    "        \"\"\"\n",
    "\n",
    "        # binary cross entropy error\n",
    "        self.bce = tf.nn.sigmoid_cross_entropy_with_logits(labels=model.x, logits=model.logits)\n",
    "        self.reconstruction_loss = tf.reduce_mean(tf.reduce_sum(self.bce, axis=-1))\n",
    "\n",
    "        if model.distribution == 'normal':\n",
    "            # KL divergence between normal approximate posterior and standard normal prior\n",
    "            self.p_z = tf.distributions.Normal(tf.zeros_like(model.z), tf.ones_like(model.z))\n",
    "            kl = model.q_z.kl_divergence(self.p_z)\n",
    "            self.kl = tf.reduce_mean(tf.reduce_sum(kl, axis=-1))\n",
    "        elif model.distribution == 'vmf':\n",
    "            # KL divergence between vMF approximate posterior and uniform hyper-spherical prior\n",
    "            self.p_z = HypersphericalUniform(model.z_dim - 1, dtype=model.x.dtype)\n",
    "            kl = model.q_z.kl_divergence(self.p_z)\n",
    "            self.kl = tf.reduce_mean(kl)\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        self.ELBO = - self.reconstruction_loss - self.kl\n",
    "\n",
    "        self.train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(-self.ELBO)\n",
    "\n",
    "        self.print = {'recon loss': self.reconstruction_loss, 'ELBO': self.ELBO, 'KL': self.kl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(model, optimizer, n=10):\n",
    "    \"\"\"\n",
    "\n",
    "    :param model: model object\n",
    "    :param optimizer: optimizer object\n",
    "    :param n: number of MC samples\n",
    "    :return: MC estimate of log-likelihood\n",
    "    \"\"\"\n",
    "\n",
    "    z = model.q_z.sample(n)\n",
    "\n",
    "    log_p_z = optimizer.p_z.log_prob(z)\n",
    "\n",
    "    if model.distribution == 'normal':\n",
    "        log_p_z = tf.reduce_sum(log_p_z, axis=-1)\n",
    "\n",
    "    log_p_x_z = -tf.reduce_sum(optimizer.bce, axis=-1)\n",
    "\n",
    "    log_q_z_x = model.q_z.log_prob(z)\n",
    "\n",
    "    if model.distribution == 'normal':\n",
    "        log_q_z_x = tf.reduce_sum(log_q_z_x, axis=-1)\n",
    "\n",
    "    return tf.reduce_mean(tf.reduce_logsumexp(\n",
    "        tf.transpose(log_p_x_z + log_p_z - log_q_z_x) - np.log(n), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hidden dimension and dimension of latent space\n",
    "H_DIM = 128\n",
    "Z_DIM = 2\n",
    "\n",
    "# digit placeholder\n",
    "x = tf.placeholder(tf.float32, shape=(None, 784))\n",
    "\n",
    "# normal VAE\n",
    "modelN = ModelVAE(x=x, h_dim=H_DIM, z_dim=Z_DIM, distribution='normal')\n",
    "optimizerN = OptimizerVAE(modelN)\n",
    "\n",
    "# hyper-spherical VAE\n",
    "modelS = ModelVAE(x=x, h_dim=H_DIM, z_dim=Z_DIM + 1, distribution='vmf')\n",
    "optimizerS = OptimizerVAE(modelS)\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Normal VAE #####\n",
      "0 {'KL': 0.16437618, 'recon loss': 539.3147, 'ELBO': -539.47906}\n",
      "100 {'KL': 2.8173559, 'recon loss': 198.61513, 'ELBO': -201.43248}\n",
      "200 {'KL': 3.7334108, 'recon loss': 183.73953, 'ELBO': -187.47295}\n",
      "300 {'KL': 3.9201128, 'recon loss': 176.92807, 'ELBO': -180.84819}\n",
      "400 {'KL': 4.118131, 'recon loss': 174.5309, 'ELBO': -178.64903}\n",
      "500 {'KL': 4.4276586, 'recon loss': 171.1067, 'ELBO': -175.53436}\n",
      "600 {'KL': 4.606196, 'recon loss': 168.9398, 'ELBO': -173.546}\n",
      "700 {'KL': 4.4683576, 'recon loss': 167.64552, 'ELBO': -172.11388}\n",
      "800 {'KL': 4.7288294, 'recon loss': 166.15912, 'ELBO': -170.88795}\n",
      "900 {'KL': 4.636322, 'recon loss': 164.90408, 'ELBO': -169.5404}\n",
      "Test set:\n",
      "{'KL': 4.4319677, 'recon loss': 165.60225, 'ELBO': -170.03421, 'LL': -168.92137}\n",
      "\n",
      "##### Hyper-spherical VAE #####\n",
      "0 {'KL': 0.31487006, 'recon loss': 540.8928, 'ELBO': -541.2077}\n",
      "100 {'KL': 1.6287001, 'recon loss': 200.90146, 'ELBO': -202.53015}\n",
      "200 {'KL': 2.736233, 'recon loss': 184.6564, 'ELBO': -187.39264}\n",
      "300 {'KL': 3.4729977, 'recon loss': 176.09352, 'ELBO': -179.56651}\n",
      "400 {'KL': 3.8302422, 'recon loss': 172.93442, 'ELBO': -176.76466}\n",
      "500 {'KL': 4.017993, 'recon loss': 168.77925, 'ELBO': -172.79724}\n",
      "600 {'KL': 4.2170806, 'recon loss': 165.37062, 'ELBO': -169.5877}\n",
      "700 {'KL': 4.374174, 'recon loss': 163.67372, 'ELBO': -168.0479}\n",
      "800 {'KL': 4.4423, 'recon loss': 160.80138, 'ELBO': -165.24368}\n",
      "900 {'KL': 4.5763006, 'recon loss': 159.21205, 'ELBO': -163.78835}\n",
      "Test set:\n",
      "{'KL': 4.6741014, 'recon loss': 158.62828, 'ELBO': -163.30238, 'LL': -162.29}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('##### Normal VAE #####')\n",
    "for i in range(1000):\n",
    "    # training\n",
    "    x_mb, _ = mnist.train.next_batch(64)\n",
    "    session.run(optimizerN.train_step, {modelN.x: x_mb})\n",
    "\n",
    "    # every 100 iteration plot validation\n",
    "    if i % 100 == 0:\n",
    "        x_mb = mnist.validation.images\n",
    "        print(i, session.run({**optimizerN.print}, {modelN.x: x_mb}))\n",
    "\n",
    "print('Test set:')\n",
    "x_mb = mnist.test.images\n",
    "print_ = {**optimizerN.print}\n",
    "print_['LL'] = log_likelihood(modelN, optimizerN, n=100)\n",
    "print(session.run(print_, {modelN.x: x_mb}))\n",
    "\n",
    "print()\n",
    "print('##### Hyper-spherical VAE #####')\n",
    "for i in range(1000):\n",
    "    # training\n",
    "    x_mb, _ = mnist.train.next_batch(64)\n",
    "    session.run(optimizerS.train_step, {modelS.x: x_mb})\n",
    "\n",
    "    # every 100 iteration plot validation\n",
    "    if i % 100 == 0:\n",
    "        x_mb = mnist.validation.images\n",
    "        print(i, session.run({**optimizerS.print}, {modelS.x: x_mb}))\n",
    "\n",
    "print('Test set:')\n",
    "x_mb = mnist.test.images\n",
    "print_ = {**optimizerS.print}\n",
    "print_['LL'] = log_likelihood(modelS, optimizerS, n=100)\n",
    "print(session.run(print_, {modelS.x: x_mb}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
